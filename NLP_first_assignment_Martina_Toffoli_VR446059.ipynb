{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **`A Naïve Bayes Classifier for Text Classification`**\n",
        "# **Candidate: *Martina Toffoli VR446059***\n",
        "\n",
        "The assignment consists in the development, through the use of NLTK, of a Naïve Bayes Classifier able to detect a single class in one of the corpora available as attachments to the chosen package, by distinguishing ENGLISH against NON-ENGLISH.\n",
        "\n",
        "This first block is related to the main libraries needed to compute the classification task. Most of the produced code is based on NLTK which is the Natural Language Toolkit used to achieve our goal."
      ],
      "metadata": {
        "id": "YRCtM3f819wo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from os import listdir\n",
        "from os.path import isdir, join\n",
        "import nltk\n",
        "nltk.download('europarl_raw')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus.europarl_raw import english, french, danish, dutch, finnish, german, greek, portuguese, italian, spanish, swedish\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import heapq\n",
        "import numpy as np\n",
        "import csv\n",
        "import random\n",
        "import string\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.metrics.confusionmatrix import ConfusionMatrix\n",
        "from nltk.metrics import recall, precision, f_measure\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "tx0q5-_H1-7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21c7c134-1f1f-47ea-b8fd-c66071703dd3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package europarl_raw to /root/nltk_data...\n",
            "[nltk_data]   Package europarl_raw is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some function have been subsequently created respectively to preprocess the text, compute the words occurrences and to extract the relevant features from the data:\n",
        "\n",
        "*   text_converter(text)\n",
        "*   freqWords(dataset, minFreq)\n",
        "*   find_features(sentence, freq_words)\n"
      ],
      "metadata": {
        "id": "xgyzWfBC3gDT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function: **text_converter(text)**\n",
        "During the analysis of a particular text, the model better understand the content of the input if some preprocessing has been done. Usually, lowerizing the input text deleting the puntcuation are classical operations to improve the performance of a machine learning model.\n",
        "We demonstrate that the capability of language text classification can be learned feeding the model with inputs that are sentences and not necessarily an entire text. For this reason we tokenize the input using a sentence tokenizer and we use each sentence as if it were a different sample text.\n",
        "This consideration, enable us to train the model with a limit amount of data and obtaining high performance. Therefore, we read a long text for each language, we apply a sentence tokenization on it obtaining a list of sentences used for training the model.\n",
        "\n",
        "**[Concept of Tokenization]:** It is the process by which a large quantity of text is divided into smaller parts called tokens.\n",
        "\n",
        "**[Input]:** text\n",
        "\n",
        "**[Output]:** list of text"
      ],
      "metadata": {
        "id": "_I0CXjam4fbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_converter(testo):\n",
        "    # STEP 1: We will first preprocess the data, in order to:\n",
        "    dataset = nltk.sent_tokenize(testo)\n",
        "    for i in range(len(dataset)):\n",
        "        # remove numbers\n",
        "        sentence_ith = re.sub(r'\\d+', '', dataset[i])\n",
        "        # remove punctuations and convert characters to lower case\n",
        "        dataset[i] = \"\".join([char.lower() for char in sentence_ith if char not in string.punctuation])\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "o_wTu79W3aG0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function: **freqWords(dataset, minFreq)**\n",
        "This procedure acts as a support function which helps us to create the Bag of Words (BOW) that is a method to extract features from text documents. In simple terms, it returns a collection of tokens which represents the occurrences of words in the text disregarding the order in which they appear. This is useful because when we analyze the content of the text, we inherently associate a weigth to each word. For this reason we decide to make the model concentrate on the most redundant words in the text. \n",
        "\n",
        "**[Input]:** text, number of different ***n*** words to consider as most relevant\n",
        "\n",
        "**[Output]:** list of most ***n*** relevant words"
      ],
      "metadata": {
        "id": "ooOJhhlf8eSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def freqWords(dataset, minFreq):\n",
        "    # STEP 2: Obtaining most frequent words in our text.\n",
        "    # Creating the Bag of Words model\n",
        "    word2count = {}\n",
        "    ps = PorterStemmer()\n",
        "    for sentence in dataset:\n",
        "        words = nltk.word_tokenize(sentence)\n",
        "        for word in words:\n",
        "            word = ps.stem(word)\n",
        "            if word not in word2count.keys():\n",
        "                word2count[word] = 1\n",
        "            else:\n",
        "                word2count[word] += 1\n",
        "    # estrapolo le 200 parole più frequenti nel testo\n",
        "    freq_words = heapq.nlargest(minFreq, word2count, key = word2count.get)\n",
        "    #print(freq_words)\n",
        "    return freq_words"
      ],
      "metadata": {
        "id": "bSwzgdjY8pcW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function: **find_features(sentence, freq_words)**\n",
        "The purpose of this function consists of extracting/finding the most relevant features in the text as explained above. In particular, providing in input the text and the list of the most ***n*** frequent words, we maintain only the words that appears in that list.\n",
        "\n",
        "**[Input]:** text, list of most frequents words\n",
        "\n",
        "**[Output]:** dictionary {word: word}"
      ],
      "metadata": {
        "id": "Xwb3eO84AU33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_features(sentence, freq_words):\n",
        "    features = {}\n",
        "    words = nltk.word_tokenize(sentence)\n",
        "    for word in words:\n",
        "        if word in freq_words:\n",
        "            features[word] = word\n",
        "    \n",
        "    return features"
      ],
      "metadata": {
        "id": "is-t8URdHh8H"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The classifier was implemented starting from the ***europarl_raw*** corpus present in the NLTK package.\n",
        "\n",
        "It contains documents written in diffent languages: French, Italian, Spanish, Portuguese, German, English, Dutch, Danish, Swedish, Finnish and Greek.\n",
        "We decide to use it because its size, number of languages and number of documents are appropriate for this specific task.\n",
        "\n",
        "All the texts of each language have been modified through the *text_converter(text)* function in order to divide it into several different texts; subsequently the most frequent words within the text were detected through the *freqWords(sentences_list,minFreq)* function; finally we extract the main features on which the classifier will have to operate applying the *find_features(sentence,frequents_words)* function. \n",
        "Therefore, for each sample we extract the relative features in the form of a dictionary as explained above. In addition, we insert it into a tuple paired with its relative label which indicates whether this sentence is in English or not (English or Not-English).\n",
        "\n",
        "Example of features:\n",
        "\n",
        "                feature = ({'the' : 'the', \n",
        "                            'pen' : 'pen',\n",
        "                             'is' : 'is', \n",
        "                             'on' : 'on',\n",
        "                            'the' : 'the',\n",
        "                          'table' : 'table',}, 'English')\n",
        "\n",
        "\n",
        "                              "
      ],
      "metadata": {
        "id": "U8obHNaDII_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_elem = 100000\n",
        "\n",
        "dataset = []\n",
        "text = ' '.join(english.words())\n",
        "sentences_list = text_converter(text)\n",
        "frequents_words = freqWords(sentences_list,200)\n",
        "features = [(find_features(sentence,frequents_words), 'English') for sentence in sentences_list]\n",
        "dataset.extend(features[:max_elem])\n",
        "\n",
        "text = ' '.join(dutch.words())\n",
        "sentences_list = text_converter(text)\n",
        "frequents_words = freqWords(sentences_list,200)\n",
        "features = [(find_features(sentence,frequents_words), 'Not-English') for sentence in sentences_list]\n",
        "dataset.extend(features[:max_elem//10])\n",
        "\n",
        "text = ' '.join(finnish.words())\n",
        "sentences_list = text_converter(text)\n",
        "frequents_words = freqWords(sentences_list,200)\n",
        "features = [(find_features(sentence,frequents_words), 'Not-English') for sentence in sentences_list]\n",
        "dataset.extend(features[:max_elem//10])\n",
        "\n",
        "text = ' '.join(german.words())\n",
        "sentences_list = text_converter(text)\n",
        "frequents_words = freqWords(sentences_list,200)\n",
        "features = [(find_features(sentence,frequents_words), 'Not-English') for sentence in sentences_list]\n",
        "dataset.extend(features[:max_elem//10])\n",
        "\n",
        "text = ' '.join(greek.words())\n",
        "sentences_list = text_converter(text)\n",
        "frequents_words = freqWords(sentences_list,200)\n",
        "features = [(find_features(sentence,frequents_words), 'Not-English') for sentence in sentences_list]\n",
        "dataset.extend(features[:max_elem//10])\n",
        "\n",
        "text = ' '.join(portuguese.words())\n",
        "sentences_list = text_converter(text)\n",
        "frequents_words = freqWords(sentences_list,200)\n",
        "features = [(find_features(sentence,frequents_words), 'Not-English') for sentence in sentences_list]\n",
        "dataset.extend(features[:max_elem//10])\n",
        "\n",
        "text = ' '.join(italian.words())\n",
        "sentences_list = text_converter(text)\n",
        "frequents_words = freqWords(sentences_list,200)\n",
        "features = [(find_features(sentence,frequents_words), 'Not-English') for sentence in sentences_list]\n",
        "dataset.extend(features[:max_elem//10])\n",
        "\n",
        "text = ' '.join(spanish.words())\n",
        "sentences_list = text_converter(text)\n",
        "frequents_words = freqWords(sentences_list,200)\n",
        "features = [(find_features(sentence,frequents_words), 'Not-English') for sentence in sentences_list]\n",
        "dataset.extend(features[:max_elem//10])\n",
        "\n",
        "text = ' '.join(swedish.words())\n",
        "sentences_list = text_converter(text)\n",
        "frequents_words = freqWords(sentences_list,200)\n",
        "features = [(find_features(sentence,frequents_words), 'Not-English') for sentence in sentences_list]\n",
        "dataset.extend(features[:max_elem//10])\n",
        "\n",
        "text = ' '.join(french.words())\n",
        "sentences_list = text_converter(text)\n",
        "frequents_words = freqWords(sentences_list,200)\n",
        "features = [(find_features(sentence,frequents_words), 'Not-English') for sentence in sentences_list]\n",
        "dataset.extend(features[:max_elem//10])\n",
        "\n",
        "text = ' '.join(danish.words())\n",
        "sentences_list = text_converter(text)\n",
        "frequents_words = freqWords(sentences_list,200)\n",
        "features = [(find_features(sentence,frequents_words), 'Not-English') for sentence in sentences_list]\n",
        "dataset.extend(features[:max_elem//10])"
      ],
      "metadata": {
        "id": "TWyb9GBIItwe"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final dataset is a set of balanced samples containing text in English and not in equal numbers; this allows the classifier to interpret more accurately whether a text is in English or not.\n",
        "\n",
        "A *random.shuffle(dataset)* operation is performed before training the model to mix all the samples so as not to have a list of ordered text by language, enabling the classifier to learn better from a non homogenous dataset.\n"
      ],
      "metadata": {
        "id": "-L9K7Epneo7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(dataset)"
      ],
      "metadata": {
        "id": "ZwvQkqPyednY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this assignment we decided to use the Naïve Bayes Classifier. In order to find the probability for a label, this algorithm first uses the Bayes rule to express P(label|features) in terms of P(label) and P(features|label):\n",
        "                     \n",
        "*P(label|features) = ( P(label) * P(features|label) ) / P(features)*\n",
        "\n",
        "Moreover, it makes use of the 'naïve' assumption that all features are\n",
        "independent as shown in the following expression:\n",
        "\n",
        "*P(label|features) = ( P(label) * P(f1|label) * ... * P(fn|label) ) / P(features)*\n",
        "\n",
        "In addition, rather than computing P(features) explicitly, the algorithm just\n",
        "calculates the numerator for each label, and normalizes them so they\n",
        "sum to one:\n",
        "\n",
        "*P(label|features) = ( P(label) * P(f1|label) * ... * P(fn|label) ) / ( SUM(l)( P(l) * P(f1|l) * ... * P(fn|l) )*\n",
        "\n",
        "To train the model, it is necessary to split the dataset into a training set and a test set: \n",
        "The training set is used to allow the algorithm to distinguish English texts with respect to other languages while the test set is useful to evaluate how well the algorithm works, namely the performance of our machine learning model.\n",
        "\n",
        "In this assignment, we use 10% of the entire dataset during the evaluation (test set) and the the remaining part for the training phase (training set)."
      ],
      "metadata": {
        "id": "PA5WHDwMYo0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set, test_set = dataset[max_elem//10:], dataset[:max_elem//10]\n",
        "classifier = nltk.NaiveBayesClassifier.train(train_set)"
      ],
      "metadata": {
        "id": "JjE-Dv81nY97"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Performance indicators**\n",
        "For this assignment it was decided to use a series of indicators that allow us to interpret the performance of the classifier, including:\n",
        "\n",
        "\n",
        "*   **Confusion Matrix**: It is crucial to verify the errors predictions and to understand where the model makes the wrong choice. \n",
        "*   **Accuracy**: Given a list of reference values and a corresponding list of test values, return the fraction of corresponding values that are equal.\n",
        "*   **Precision**: Given a set of reference values and a set of test values, return the fraction of test values that appear in the reference set.\n",
        "*   **Recall**: Given a set of reference values and a set of test values, return the fraction of reference values that appear in the test set.\n",
        "*   **F-Measure**:  Given a set of reference values and a set of test values, return the f-measure of the test values, when compared against the reference values.  The f-measure is the harmonic mean of the precision and recall.\n"
      ],
      "metadata": {
        "id": "o1yB1_t0neLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "groundtruth = [ sample[1] for sample in test_set]\n",
        "predictions = []\n",
        "for sample, _ in test_set:\n",
        "    prediction = classifier.classify(sample)\n",
        "    predictions.append(prediction)\n",
        "\n",
        "print(\"Confusion Matrix:\\n\")\n",
        "print(ConfusionMatrix(groundtruth, predictions))\n",
        "\n",
        "groundtruth, predictions = defaultdict(set), defaultdict(set)\n",
        "\n",
        "for i, (features, label) in enumerate(test_set):\n",
        "    predictions[classifier.classify(features)].add(i)\n",
        "    groundtruth[label].add(i) \n",
        "print(\" Accuracy:\", nltk.classify.accuracy(classifier, test_set))\n",
        "print(\"Precision:\", precision(groundtruth[label], predictions[label]))\n",
        "print(\"   Recall:\", recall(groundtruth[label], predictions[label]))\n",
        "print(\"F-Measure:\", f_measure(groundtruth[label], predictions[label]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DauTbpo5o9qm",
        "outputId": "cc51ef31-17bd-474d-a285-8944fe56b089"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "\n",
            "            |         N |\n",
            "            |         o |\n",
            "            |         t |\n",
            "            |         - |\n",
            "            |    E    E |\n",
            "            |    n    n |\n",
            "            |    g    g |\n",
            "            |    l    l |\n",
            "            |    i    i |\n",
            "            |    s    s |\n",
            "            |    h    h |\n",
            "------------+-----------+\n",
            "    English |<1624>   4 |\n",
            "Not-English |    2<8370>|\n",
            "------------+-----------+\n",
            "(row = reference; col = test)\n",
            "\n",
            " Accuracy: 0.9994\n",
            "Precision: 0.9995223310246\n",
            "   Recall: 0.9997611084567606\n",
            "F-Measure: 0.9996417054819061\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**\n",
        "We demostrated to obtain optimal results.\n",
        "Considerations: \n",
        "*   This is due to a large amount of data, namely the quantity of english features is equal of the quantity of the not english features. \n",
        "*   The use of several and different metrics is a good indicator for the perfomance of the model. Therefore we use not only precision but also recall, accuracy and f-measure.\n",
        "*   As shown in the confusion matrix, just only very few texts are wrong classify.\n",
        "*   We almost reach the optimal point.\n",
        "\n",
        "# **Future Consideration**\n",
        "*   Different tokenizer could be tested.\n",
        "*   The function used to extract the features can be enhance using not just the most relevant words but also some chunks of the text, avoiding stopwords and so on...\n",
        "*   A different classifier could be used in order to analyze the strenghts and weaknesses of different models.\n",
        "*   It would be interesing to analyze what happens to the model performance to vary the amount of data used to feed the model during the training phase.\n",
        "\n"
      ],
      "metadata": {
        "id": "8KpXrWfxVAw_"
      }
    }
  ]
}